#!/usr/bin/env python3
"""
Debug KT analysis mapping issue
"""
import sys
sys.path.insert(0, './src')

def debug_kt_mapping():
    """Debug what keys are generated for KT analysis"""
    
    try:
        from app import RCAApp
        
        print("=== Debugging KT Analysis Key Mapping ===\n")
        
        app = RCAApp()
        
        # Simulate what you saw in the browser
        print("CURRENT SITUATION:")
        print("Browser shows:")
        print("- Root Cause: Corruption in SliceInfo data within Zookeeper...")
        print("- Additional Sections: Cpe, CPE-9501")
        print()
        
        # Mock the analysis result based on what you described
        simulated_analysis = {
            'prompt_file_used': 'kt-analysis_prompt',
            'analysis': {
                'sources_used': ['Jira Ticket: CPE-9501', 'Jira Ticket: ELEM-17246', 'Jira Ticket: ELEM-17254', 'Jira Ticket: ELEM-17244'],
                'root_cause': 'Corruption in SliceInfo data within Zookeeper causing API failures.',
                'cpe': 'CPE-9501',  # This is what showed in "Additional Sections"
                # These might be the actual keys generated by the LLM:
                'problem_statement': 'Some problem statement text',
                'problem_analysis': 'Some problem analysis text',
                'potential_causes': 'Some potential causes text',
                'solution_development': 'Some solution development text',
            }
        }
        
        prompt_file = simulated_analysis['prompt_file_used']
        analysis = simulated_analysis['analysis']
        
        print(f"Prompt file: {prompt_file}")
        print(f"Analysis keys: {list(analysis.keys())}")
        print()
        
        # Check predefined mapping
        section_mapping = app.PROMPT_REPORT_MAP.get(prompt_file, [])
        print(f"Predefined mapping ({len(section_mapping)} sections):")
        for header, key in section_mapping:
            print(f"  '{header}' -> '{key}'")
        print()
        
        # Check what would be found/missing
        print("MAPPING RESULTS:")
        found_sections = []
        missing_sections = []
        
        for header, expected_key in section_mapping:
            if expected_key in analysis:
                found_sections.append((header, expected_key))
                print(f"✅ {header} -> {expected_key} (FOUND)")
            else:
                missing_sections.append((header, expected_key))
                print(f"❌ {header} -> {expected_key} (MISSING)")
        
        print(f"\nFound: {len(found_sections)}/{len(section_mapping)} sections")
        
        # Check unmapped keys
        mapped_keys = [key for _, key in section_mapping]
        unmapped_keys = [k for k in analysis.keys() if k not in mapped_keys and k not in ['sources_used', 'raw_response', 'raw_analysis']]
        
        print(f"\nUnmapped keys that would go to 'Additional Sections': {unmapped_keys}")
        
        print("\n" + "="*60)
        print("RECOMMENDATIONS:")
        print("1. Check the actual LLM response to see what keys it generates")
        print("2. Update PROMPT_REPORT_MAP to match actual LLM output")
        print("3. Improve fuzzy matching for KT numbered sections")
        
        return True
        
    except Exception as e:
        print(f"❌ Debug failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    debug_kt_mapping()
